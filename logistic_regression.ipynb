{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraries"
      ],
      "metadata": {
        "id": "9bEguAGgFUbT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFqfv6EgCr2j"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import numpy  as np\n",
        "import string\n",
        "import re\n",
        "\n",
        "#NLP\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import spacy\n",
        "from spacy.symbols import nsubj, VERB\n",
        "nlp = spacy.load(\"nl_core_news_sm\")\n",
        "#from spacy.lang.nl.stop_words import STOP_WORDS\n",
        "\n",
        "# Model implementation\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading datasets"
      ],
      "metadata": {
        "id": "u5w8Ep6eFSnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"classification_cleaned_data.csv\")\n",
        "\n",
        "df_test_q = pd.read_csv(\"question_training_dataset.csv\")\n",
        "df_test_c = pd.read_csv(\"concern_training_dataset.csv\")\n",
        "df_test_d = pd.read_csv(\"doubt_training_dataset.csv\")"
      ],
      "metadata": {
        "id": "pyKkkr25CyVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into training and remaining data\n",
        "train_data, remaining_data = train_test_split(df, test_size=0.4, random_state=42)  # 60% training, 40% remaining\n",
        "val_data, test_data = train_test_split(remaining_data, test_size=0.5, random_state=42)  # 20% validation, 20% test"
      ],
      "metadata": {
        "id": "TJZ2Msn-DzDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading pre trained embedding model"
      ],
      "metadata": {
        "id": "Nr3VBjlmFXZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretrained embedding model\n",
        "model_path = '/home/bekkalim/Documents/dutch-word-embeddings/model.bin'\n",
        "\n",
        "# Load the pre-trained Word2Vec model\n",
        "model = KeyedVectors.load_word2vec_format(model_path, binary=True)"
      ],
      "metadata": {
        "id": "X4AldCG1D0pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to change text into vector with normalization\n",
        "def text_to_vector_norm(text):\n",
        "    words = text.split()  # Tokenize the text string into words\n",
        "    word_vectors = []\n",
        "\n",
        "    for word in words:\n",
        "        if word in model.key_to_index:  # Check if the word is in the model's vocabulary\n",
        "            word_vectors.append(model[word])\n",
        "\n",
        "    if word_vectors:  # If there is at least one word vector, calculate the mean vector\n",
        "        document_vector = np.mean(word_vectors, axis=0)\n",
        "    else:  # If no words in the text are in the model's vocabulary, return a zero vector\n",
        "        document_vector = np.zeros(model.vector_size)\n",
        "\n",
        "    # Normalize the document vector to unit norm\n",
        "    document_vector = normalize([document_vector])[0]\n",
        "\n",
        "    return document_vector"
      ],
      "metadata": {
        "id": "IuWsIGT8D39-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if data is a string\n",
        "train_data['clean_text'] = train_data['clean_text'].astype(str)\n",
        "test_data['clean_text'] = test_data['clean_text'].astype(str)\n",
        "val_data['clean_text'] = val_data['clean_text'].astype(str)"
      ],
      "metadata": {
        "id": "mXjifY1XD7Jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying function to every dataset\n",
        "train_data['text_vector'] = train_data['clean_text'].apply(text_to_vector_norm)\n",
        "test_data['text_vector'] = test_data['clean_text'].apply(text_to_vector_norm)\n",
        "val_data['text_vector'] = val_data['clean_text'].apply(text_to_vector_norm)\n",
        "\n",
        "df_test_q['text_vector'] = df_test_q['clean_text'].apply(text_to_vector_norm)\n",
        "df_test_c['text_vector'] = df_test_c['clean_text'].apply(text_to_vector_norm)\n",
        "df_test_d['text_vector'] = df_test_d['clean_text'].apply(text_to_vector_norm)"
      ],
      "metadata": {
        "id": "FLNv6FNZD9kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oversampling"
      ],
      "metadata": {
        "id": "Z3PHP5qbFzsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing smote oversampling\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_q_res, y_train_q_res = smote.fit_resample(X_train, y_train_q)\n",
        "X_train_c_res, y_train_c_res = smote.fit_resample(X_train, y_train_c)\n",
        "X_train_d_res, y_train_d_res = smote.fit_resample(X_train, y_train_d)\n",
        "\n",
        "# Convert the result to a pandas Series to use value_counts()\n",
        "y_train_q_res_series = pd.Series(y_train_q_res)\n",
        "y_train_c_res_series = pd.Series(y_train_c_res)\n",
        "y_train_d_res_series = pd.Series(y_train_d_res)\n",
        "\n",
        "# Count the occurrences of each class\n",
        "class_counts_q = y_train_q_res_series.value_counts()\n",
        "print(class_counts_q)\n",
        "\n",
        "# Count the occurrences of each class\n",
        "class_counts_c = y_train_c_res_series.value_counts()\n",
        "print(class_counts_c)\n",
        "\n",
        "# Count the occurrences of each class\n",
        "class_counts_d = y_train_d_res_series.value_counts()\n",
        "print(class_counts_d)\n"
      ],
      "metadata": {
        "id": "kwj-e_dZEAdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for gridseach on parameter tuning\n",
        "def grid_search_and_evaluate_logreg_2(X_train, y_train, X_test, y_test, target_name):\n",
        "    # Defining the parameter grid for grid search\n",
        "    param_grid = {\n",
        "        'C': [0.01, 0.1, 1.0, 10.0, 100.0],  # Inverse of regularization strength\n",
        "        'penalty': ['l2'],\n",
        "        'solver': ['lbfgs']\n",
        "    }\n",
        "\n",
        "    # Set up the grid search with cross-validation\n",
        "    grid_search = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='f1_macro')\n",
        "\n",
        "    # Fit the grid search on the resampled training data\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"Best parameters for {target_name}: \", grid_search.best_params_)\n",
        "\n",
        "    # Train the best model\n",
        "    best_logreg_model = grid_search.best_estimator_\n",
        "    best_logreg_model.fit(X_train, y_train)\n",
        "\n",
        "    # Perform cross-validation on the resampled training data\n",
        "    cv_scores = cross_val_score(best_logreg_model, X_train, y_train, cv=10, scoring='f1_macro')\n",
        "    print(f\"Cross-validation scores for {target_name}: {cv_scores}\")\n",
        "    print(f\"Mean cross-validation score for {target_name}: {np.mean(cv_scores)}\")\n",
        "\n",
        "    # Predict and evaluate on the test set\n",
        "    y_preds = best_logreg_model.predict(X_test)\n",
        "    print(f\"Classification report for {target_name}:\")\n",
        "    print(classification_report(y_test, y_preds, digits=4))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_preds)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['False', 'True'], yticklabels=['False', 'True'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title(f'Confusion Matrix for {target_name}')\n",
        "    plt.show()\n",
        "\n",
        "    # Identify false positives and false negatives\n",
        "    false_positives = np.where((y_preds == 1) & (y_test == 0))[0]\n",
        "    false_negatives = np.where((y_preds == 0) & (y_test == 1))[0]\n",
        "\n",
        "    # Assuming X_test_original is a DataFrame or Series\n",
        "    X_test_list = X_test_original.tolist()\n",
        "\n",
        "    # Create DataFrame for test data\n",
        "    df_test = pd.DataFrame({'clean_text': X_test_list, 'actual': y_test, 'predicted': y_preds})\n",
        "\n",
        "    # Extract false positives and false negatives\n",
        "    false_positive_samples = df_test.iloc[false_positives]\n",
        "    false_negative_samples = df_test.iloc[false_negatives]\n",
        "\n",
        "    # Display 10 of the false positives\n",
        "    print(\"False Positives:\")\n",
        "    print(false_positive_samples.head(10))\n",
        "\n",
        "    # Display 10 of the false negatives\n",
        "    print(\"\\nFalse Negatives:\")\n",
        "    print(false_negative_samples.head(10))\n",
        "\n",
        "    # Plotting cross-validation scores\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, len(cv_scores) + 1), cv_scores, marker='o', linestyle='--', color='b', label='Cross-Validation F1 Score')\n",
        "    plt.axhline(np.mean(cv_scores), color='r', linestyle='-', label=f'Mean F1 Score: {np.mean(cv_scores):.4f}')\n",
        "    plt.title(f'Cross-Validation F1 Scores for {target_name}')\n",
        "    plt.xlabel('Fold')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return best_logreg_model\n"
      ],
      "metadata": {
        "id": "t_TxWu0qED0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search_and_evaluate_logreg_2(X_train_q_res, y_train_q_res, X_test_manual_q, y_test_manual_q, \"is_question\")\n",
        "grid_search_and_evaluate_logreg_2(X_train_c_res, y_train_c_res, X_test_manual_c, y_test_manual_c, \"is_concern\")\n",
        "grid_search_and_evaluate_logreg_2(X_train_d_res, y_train_d_res, X_test_manual_d, y_test_manual_d, \"is_doubt\")"
      ],
      "metadata": {
        "id": "rqoKCAlCESc5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}