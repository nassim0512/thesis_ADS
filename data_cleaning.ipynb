{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Download and install packages"
      ],
      "metadata": {
        "id": "tbFLGQoXVhdz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "705CLCOyVQV7"
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download nl_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraries"
      ],
      "metadata": {
        "id": "jUh3ZpeMVogc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy  as np\n",
        "import string\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "#NLP\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "import spacy\n",
        "from spacy.symbols import nsubj, VERB\n",
        "from spacy.lang.nl.stop_words import STOP_WORDS\n",
        "\n",
        "nlp = spacy.load(\"nl_core_news_sm\")\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "SEDKvmyyVet3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning"
      ],
      "metadata": {
        "id": "vwzPM1ndWQ4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# additional stopwords obtained from https://github.com/stopwords-iso/stopwords-nl/blob/master/stopwords-nl.txt\n",
        "nl_stopwords = pd.read_csv('stopwords-nl.txt')\n",
        "# flatten list and strip spaces\n",
        "nl_stopwords = [item.strip() for sublist in nl_stopwords.values.tolist() for item in sublist]"
      ],
      "metadata": {
        "id": "RI34i3o6Vre3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading dataset\n",
        "df_complete = pd.read_pickle(\"data_randomized_complete_correct.pkl\")"
      ],
      "metadata": {
        "id": "5or3Y9R8VuHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply to full_text column\n",
        "df_complete = df_complete.dropna(subset=['full_text'])\n",
        "\n",
        "# Remove rows where the length of text in 'text' column is less than 5 characters\n",
        "df_complete = df_complete[df_complete['full_text'].str.len() >= 5]\n",
        "\n",
        "# Remove rows where the text is exactly \"Nee, ik heb geen opmerkingen\"\n",
        "df_complete = df_complete[df_complete['full_text'] != \"Nee, ik heb geen opmerkingen\"]"
      ],
      "metadata": {
        "id": "zxzCJCa7VxNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Dutch NLP model\n",
        "nlp = spacy.load(\"nl_core_news_sm\")\n",
        "\n",
        "# Update Spacy's default stop words with additional custom Dutch stopwords\n",
        "for word in nl_stopwords:\n",
        "    nlp.Defaults.stop_words.add(word)\n",
        "\n",
        "english_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "nlp.Defaults.stop_words.update(english_stopwords)\n",
        "\n",
        "def clean_and_lemmatize(text):\n",
        "    # Lowercase the text to normalize it\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove new line characters and other unnecessary whitespaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Remove all numbers (digits)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Process the cleaned text through Spacy, filtering out stop words and punctuation\n",
        "    doc = nlp(text)\n",
        "    cleaned_text = ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
        "\n",
        "    return cleaned_text"
      ],
      "metadata": {
        "id": "FhQWgv7gVzv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_complete['clean_text'] = df_complete['full_text'].apply(clean_and_lemmatize)"
      ],
      "metadata": {
        "id": "SJsA6wZvWDyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving as pickle\n",
        "df_complete.to_pickle('data_randomized_complete_cleaned.pkl')"
      ],
      "metadata": {
        "id": "3BraQqbRWEVM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}