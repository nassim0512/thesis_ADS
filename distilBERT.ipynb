{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraries"
      ],
      "metadata": {
        "id": "e9gGtDl1JBdz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82EKvGi8Gumd"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import numpy  as np\n",
        "import string\n",
        "import re\n",
        "\n",
        "#NLP\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import spacy\n",
        "from spacy.symbols import nsubj, VERB\n",
        "nlp = spacy.load(\"nl_core_news_sm\")\n",
        "#from spacy.lang.nl.stop_words import STOP_WORDS\n",
        "\n",
        "# model implementation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "import tensorflow as tf\n",
        "from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading datasets"
      ],
      "metadata": {
        "id": "fiLoFUIOJEVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading dataset\n",
        "df = pd.read_csv(\"classification_cleaned_data.csv\")\n",
        "\n",
        "# Loading test sets with manual labels\n",
        "df_test_q = pd.read_csv(\"question_training_dataset.csv\")\n",
        "df_test_c = pd.read_csv(\"concern_training_dataset.csv\")\n",
        "df_test_d = pd.read_csv(\"doubt_training_dataset.csv\")"
      ],
      "metadata": {
        "id": "95OE0IokHXTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into training and remaining data\n",
        "train_data, remaining_data = train_test_split(df, test_size=0.4, random_state=42)  # 60% training, 40% remaining\n",
        "val_data, test_data = train_test_split(remaining_data, test_size=0.5, random_state=42)  # 20% validation, 20% test"
      ],
      "metadata": {
        "id": "QZOSyiBqHeCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if everything is string\n",
        "train_data['clean_text'] = train_data['clean_text'].astype(str)\n",
        "test_data['clean_text'] = test_data['clean_text'].astype(str)\n",
        "val_data['clean_text'] = val_data['clean_text'].astype(str)"
      ],
      "metadata": {
        "id": "mf7cquqvHi9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taking samples for model training due to computational constraints"
      ],
      "metadata": {
        "id": "wIn9w6ZiJjux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample 50 True values for is_question\n",
        "sample_question = df[df['is_question'] == True].sample(n=50, random_state=42)\n",
        "\n",
        "# Sample 50 True values for is_concern\n",
        "sample_concern = df[df['is_concern'] == True].sample(n=50, random_state=42)\n",
        "\n",
        "# Sample 50 True values for is_doubt\n",
        "sample_doubt = df[df['is_doubt'] == True].sample(n=50, random_state=42)\n",
        "\n",
        "# Combine the samples into one dataframe\n",
        "combined_sample = pd.concat([sample_question, sample_concern, sample_doubt])\n",
        "\n",
        "# Shuffle the combined dataframe\n",
        "train_sample = combined_sample.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Display the combined sample\n",
        "print(train_sample)"
      ],
      "metadata": {
        "id": "dBbVKbEMHmj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying the split on the residual data"
      ],
      "metadata": {
        "id": "6G90R6QVJpNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the dataset into training and remaining data\n",
        "train_data, val_data = train_test_split(train_sample, test_size=0.2, random_state=42)  # 60% training, 40% remaining"
      ],
      "metadata": {
        "id": "DCQ4pSq9HnUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking datatype\n",
        "df_test_q['clean_text'] = df_test_q['clean_text'].astype(str)\n",
        "df_test_c['clean_text'] = df_test_c['clean_text'].astype(str)\n",
        "df_test_d['clean_text'] = df_test_d['clean_text'].astype(str)\n",
        "train_data['clean_text'] = train_data['clean_text'].astype(str)\n",
        "val_data['clean_text'] = val_data['clean_text'].astype(str)"
      ],
      "metadata": {
        "id": "FbA5am9wHssq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieving the correct datatypes to implement in the model\n",
        "X_train= np.stack(train_data['clean_text'].values)\n",
        "y_train_q = train_data['is_question'].values\n",
        "y_train_c = train_data['is_concern'].values\n",
        "y_train_d = train_data['is_doubt'].values\n",
        "\n",
        "X_test_q = np.stack(df_test_q['clean_text'].values)\n",
        "X_test_c = np.stack(df_test_c['clean_text'].values)\n",
        "X_test_d = np.stack(df_test_d['clean_text'].values)\n",
        "y_test_q = df_test_q['is_question'].values\n",
        "y_test_c = df_test_c['is_concern'].values\n",
        "y_test_d = df_test_d['is_doubt'].values\n",
        "\n",
        "X_val = np.stack(val_data['clean_text'].values)\n",
        "y_val_q = val_data['is_question'].values\n",
        "y_val_c = val_data['is_concern'].values\n",
        "y_val_d = val_data['is_doubt'].values"
      ],
      "metadata": {
        "id": "WsKebLkUHxFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question model"
      ],
      "metadata": {
        "id": "gJ0RjXV-KDnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
        "\n",
        "# Function to create dataset\n",
        "def create_dataset(encodings, labels, batch_size=16, shuffle=False):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((dict(encodings), labels))\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(1000)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "# Implementing K-Fold Cross-Validation\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Store results\n",
        "fold_metrics = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
        "    print(f\"Fold {fold + 1}\")\n",
        "\n",
        "    # Split data\n",
        "    X_train_fold = X_train[train_idx]\n",
        "    X_val_fold = X_train[val_idx]\n",
        "    y_train_fold = y_train_q[train_idx]\n",
        "    y_val_fold = y_train_q[val_idx]\n",
        "\n",
        "    # Tokenize data\n",
        "    train_encodings_fold = tokenizer(list(X_train_fold), truncation=True, padding=True, max_length=128)\n",
        "    val_encodings_fold = tokenizer(list(X_val_fold), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "    # Create TensorFlow datasets\n",
        "    train_dataset_fold = create_dataset(train_encodings_fold, y_train_fold, shuffle=True)\n",
        "    val_dataset_fold = create_dataset(val_encodings_fold, y_val_fold, batch_size=64)\n",
        "\n",
        "    # Reset the model for each fold\n",
        "    model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-multilingual-cased')\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    # Add early stopping callback\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, min_delta=0.001, restore_best_weights=True)\n",
        "    ]\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(train_dataset_fold,\n",
        "                        validation_data=val_dataset_fold,\n",
        "                        epochs=5,\n",
        "                        callbacks=callbacks)\n",
        "\n",
        "    # Store metrics\n",
        "    fold_metrics.append({\n",
        "        'train_loss': history.history['loss'],\n",
        "        'val_loss': history.history['val_loss'],\n",
        "        'train_accuracy': history.history['accuracy'],\n",
        "        'val_accuracy': history.history['val_accuracy']\n",
        "    })\n",
        "\n",
        "# Plotting function\n",
        "def plot_metrics(metrics, metric_name):\n",
        "    for fold, metric in enumerate(metrics):\n",
        "        plt.plot(metric, label=f'Fold {fold + 1}')\n",
        "    plt.title(f'{metric_name} per Fold')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Extract and plot the metrics\n",
        "train_loss_per_fold = [m['train_loss'] for m in fold_metrics]\n",
        "val_loss_per_fold = [m['val_loss'] for m in fold_metrics]\n",
        "train_accuracy_per_fold = [m['train_accuracy'] for m in fold_metrics]\n",
        "val_accuracy_per_fold = [m['val_accuracy'] for m in fold_metrics]\n",
        "\n",
        "plot_metrics(train_loss_per_fold, 'Training Loss')\n",
        "plot_metrics(val_loss_per_fold, 'Validation Loss')\n",
        "plot_metrics(train_accuracy_per_fold, 'Training Accuracy')\n",
        "plot_metrics(val_accuracy_per_fold, 'Validation Accuracy')\n",
        "\n",
        "# Evaluation on the test set\n",
        "X_test_q = np.stack(df_test_q['clean_text'].values)\n",
        "y_test_q = df_test_q['is_question'].values\n",
        "\n",
        "# Tokenize the test data\n",
        "test_texts = list(X_test_q)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n",
        "test_dataset_q = create_dataset(test_encodings, y_test_q, batch_size=64)\n",
        "\n",
        "# Create and compile the final model for testing\n",
        "final_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-multilingual-cased')\n",
        "\n",
        "# Compile the final model\n",
        "final_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "                    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "# Train the final model on the entire training set\n",
        "final_train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=128)\n",
        "final_train_dataset = create_dataset(final_train_encodings, y_train_q, shuffle=True)\n",
        "final_model.fit(final_train_dataset, epochs=5, callbacks=callbacks)\n",
        "\n",
        "# Evaluate the final model on the test set\n",
        "logits = final_model.predict(test_dataset_q)\n",
        "y_preds_q = np.argmax(logits.logits, axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(\"DistilBERT Classification Report:\")\n",
        "print(classification_report(y_test_q, y_preds_q))\n",
        "\n",
        "# Plot confusion matrix\n",
        "cm_q = confusion_matrix(y_test_q, y_preds_q)\n",
        "sns.heatmap(cm_q, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('DistilBERT Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Identify false positives and false negatives\n",
        "false_positives = np.where((y_preds_q == 1) & (y_test_q == 0))[0]\n",
        "false_negatives = np.where((y_preds_q == 0) & (y_test_q == 1))[0]\n",
        "\n",
        "print(\"\\nFalse Positives:\")\n",
        "for idx in false_positives:\n",
        "    print(test_texts[idx])\n",
        "\n",
        "print(\"\\nFalse Negatives:\")\n",
        "for idx in false_negatives:\n",
        "    print(test_texts[idx])"
      ],
      "metadata": {
        "id": "7Hi_jUX0H0wW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Concern model"
      ],
      "metadata": {
        "id": "w1cyZR-HKGUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
        "\n",
        "# Function to create dataset\n",
        "def create_dataset(encodings, labels, batch_size=16, shuffle=False):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((dict(encodings), labels))\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(1000)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "# Implementing K-Fold Cross-Validation\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Store results\n",
        "fold_metrics = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
        "    print(f\"Fold {fold + 1}\")\n",
        "\n",
        "    # Split data\n",
        "    X_train_fold = X_train[train_idx]\n",
        "    X_val_fold = X_train[val_idx]\n",
        "    y_train_fold = y_train_q[train_idx]\n",
        "    y_val_fold = y_train_q[val_idx]\n",
        "\n",
        "    # Tokenize data\n",
        "    train_encodings_fold = tokenizer(list(X_train_fold), truncation=True, padding=True, max_length=128)\n",
        "    val_encodings_fold = tokenizer(list(X_val_fold), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "    # Create TensorFlow datasets\n",
        "    train_dataset_fold = create_dataset(train_encodings_fold, y_train_fold, shuffle=True)\n",
        "    val_dataset_fold = create_dataset(val_encodings_fold, y_val_fold, batch_size=64)\n",
        "\n",
        "    # Reset the model for each fold\n",
        "    model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-multilingual-cased')\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    # Add early stopping callback\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, min_delta=0.001, restore_best_weights=True)\n",
        "    ]\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(train_dataset_fold,\n",
        "                        validation_data=val_dataset_fold,\n",
        "                        epochs=5,\n",
        "                        callbacks=callbacks)\n",
        "\n",
        "    # Store metrics\n",
        "    fold_metrics.append({\n",
        "        'train_loss': history.history['loss'],\n",
        "        'val_loss': history.history['val_loss'],\n",
        "        'train_accuracy': history.history['accuracy'],\n",
        "        'val_accuracy': history.history['val_accuracy']\n",
        "    })\n",
        "\n",
        "# Plotting function\n",
        "def plot_metrics(metrics, metric_name):\n",
        "    for fold, metric in enumerate(metrics):\n",
        "        plt.plot(metric, label=f'Fold {fold + 1}')\n",
        "    plt.title(f'{metric_name} per Fold')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Extract and plot the metrics\n",
        "train_loss_per_fold = [m['train_loss'] for m in fold_metrics]\n",
        "val_loss_per_fold = [m['val_loss'] for m in fold_metrics]\n",
        "train_accuracy_per_fold = [m['train_accuracy'] for m in fold_metrics]\n",
        "val_accuracy_per_fold = [m['val_accuracy'] for m in fold_metrics]\n",
        "\n",
        "plot_metrics(train_loss_per_fold, 'Training Loss')\n",
        "plot_metrics(val_loss_per_fold, 'Validation Loss')\n",
        "plot_metrics(train_accuracy_per_fold, 'Training Accuracy')\n",
        "plot_metrics(val_accuracy_per_fold, 'Validation Accuracy')\n",
        "\n",
        "# Evaluation on the test set\n",
        "X_test_c = np.stack(df_test_c['clean_text'].values)\n",
        "y_test_c = df_test_c['is_concern'].values\n",
        "\n",
        "# Tokenize the test data\n",
        "test_texts = list(X_test_c)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n",
        "test_dataset_c = create_dataset(test_encodings, y_test_c, batch_size=64)\n",
        "\n",
        "# Create and compile the final model for testing\n",
        "final_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-multilingual-cased')\n",
        "\n",
        "# Compile the final model\n",
        "final_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "                    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "# Train the final model on the entire training set\n",
        "final_train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=128)\n",
        "final_train_dataset = create_dataset(final_train_encodings, y_train_c, shuffle=True)\n",
        "final_model.fit(final_train_dataset, epochs=5, callbacks=callbacks)\n",
        "\n",
        "# Evaluate the final model on the test set\n",
        "logits = final_model.predict(test_dataset_c)\n",
        "y_preds_c = np.argmax(logits.logits, axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(\"DistilBERT Classification Report:\")\n",
        "print(classification_report(y_test_c, y_preds_c))\n",
        "\n",
        "# Plot confusion matrix\n",
        "cm_c = confusion_matrix(y_test_c, y_preds_c)\n",
        "sns.heatmap(cm_c, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('DistilBERT Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Identify false positives and false negatives\n",
        "false_positives = np.where((y_preds_c == 1) & (y_test_c == 0))[0]\n",
        "false_negatives = np.where((y_preds_c == 0) & (y_test_c == 1))[0]\n",
        "\n",
        "print(\"\\nFalse Positives:\")\n",
        "for idx in false_positives:\n",
        "    print(test_texts[idx])\n",
        "\n",
        "print(\"\\nFalse Negatives:\")\n",
        "for idx in false_negatives:\n",
        "    print(test_texts[idx])"
      ],
      "metadata": {
        "id": "pY4BlntRIIRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doubt model"
      ],
      "metadata": {
        "id": "l435RqmLKIcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
        "\n",
        "# Function to create dataset\n",
        "def create_dataset(encodings, labels, batch_size=16, shuffle=False):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((dict(encodings), labels))\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(1000)\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n",
        "\n",
        "# Implementing K-Fold Cross-Validation\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Store results\n",
        "fold_metrics = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
        "    print(f\"Fold {fold + 1}\")\n",
        "\n",
        "    # Split data\n",
        "    X_train_fold = X_train[train_idx]\n",
        "    X_val_fold = X_train[val_idx]\n",
        "    y_train_fold = y_train_q[train_idx]\n",
        "    y_val_fold = y_train_q[val_idx]\n",
        "\n",
        "    # Tokenize data\n",
        "    train_encodings_fold = tokenizer(list(X_train_fold), truncation=True, padding=True, max_length=128)\n",
        "    val_encodings_fold = tokenizer(list(X_val_fold), truncation=True, padding=True, max_length=128)\n",
        "\n",
        "    # Create TensorFlow datasets\n",
        "    train_dataset_fold = create_dataset(train_encodings_fold, y_train_fold, shuffle=True)\n",
        "    val_dataset_fold = create_dataset(val_encodings_fold, y_val_fold, batch_size=64)\n",
        "\n",
        "    # Reset the model for each fold\n",
        "    model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-multilingual-cased')\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    # Add early stopping callback\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, min_delta=0.001, restore_best_weights=True)\n",
        "    ]\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(train_dataset_fold,\n",
        "                        validation_data=val_dataset_fold,\n",
        "                        epochs=5,\n",
        "                        callbacks=callbacks)\n",
        "\n",
        "    # Store metrics\n",
        "    fold_metrics.append({\n",
        "        'train_loss': history.history['loss'],\n",
        "        'val_loss': history.history['val_loss'],\n",
        "        'train_accuracy': history.history['accuracy'],\n",
        "        'val_accuracy': history.history['val_accuracy']\n",
        "    })\n",
        "\n",
        "# Plotting function\n",
        "def plot_metrics(metrics, metric_name):\n",
        "    for fold, metric in enumerate(metrics):\n",
        "        plt.plot(metric, label=f'Fold {fold + 1}')\n",
        "    plt.title(f'{metric_name} per Fold')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Extract and plot the metrics\n",
        "train_loss_per_fold = [m['train_loss'] for m in fold_metrics]\n",
        "val_loss_per_fold = [m['val_loss'] for m in fold_metrics]\n",
        "train_accuracy_per_fold = [m['train_accuracy'] for m in fold_metrics]\n",
        "val_accuracy_per_fold = [m['val_accuracy'] for m in fold_metrics]\n",
        "\n",
        "plot_metrics(train_loss_per_fold, 'Training Loss')\n",
        "plot_metrics(val_loss_per_fold, 'Validation Loss')\n",
        "plot_metrics(train_accuracy_per_fold, 'Training Accuracy')\n",
        "plot_metrics(val_accuracy_per_fold, 'Validation Accuracy')\n",
        "\n",
        "# Evaluation on the test set\n",
        "X_test_d = np.stack(df_test_d['clean_text'].values)\n",
        "y_test_d = df_test_d['is_doubt'].values\n",
        "\n",
        "# Tokenize the test data\n",
        "test_texts = list(X_test_d)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n",
        "test_dataset_d = create_dataset(test_encodings, y_test_d, batch_size=64)\n",
        "\n",
        "# Create and compile the final model for testing\n",
        "final_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-multilingual-cased')\n",
        "\n",
        "# Compile the final model\n",
        "final_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
        "                    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "# Train the final model on the entire training set\n",
        "final_train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=128)\n",
        "final_train_dataset = create_dataset(final_train_encodings, y_train_d, shuffle=True)\n",
        "final_model.fit(final_train_dataset, epochs=5, callbacks=callbacks)\n",
        "\n",
        "# Evaluate the final model on the test set\n",
        "logits = final_model.predict(test_dataset_d)\n",
        "y_preds_d = np.argmax(logits.logits, axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(\"DistilBERT Classification Report:\")\n",
        "print(classification_report(y_test_d, y_preds_d))\n",
        "\n",
        "# Plot confusion matrix\n",
        "cm_d = confusion_matrix(y_test_d, y_preds_d)\n",
        "sns.heatmap(cm_d, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('DistilBERT Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Identify false positives and false negatives\n",
        "false_positives = np.where((y_preds_d == 1) & (y_test_d == 0))[0]\n",
        "false_negatives = np.where((y_preds_d == 0) & (y_test_d == 1))[0]\n",
        "\n",
        "print(\"\\nFalse Positives:\")\n",
        "for idx in false_positives:\n",
        "    print(test_texts[idx])\n",
        "\n",
        "print(\"\\nFalse Negatives:\")\n",
        "for idx in false_negatives:\n",
        "    print(test_texts[idx])"
      ],
      "metadata": {
        "id": "17eOEWNRIi95"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}