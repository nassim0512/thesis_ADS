{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraries"
      ],
      "metadata": {
        "id": "RXIceH1po5X4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nk_E62U9ogAn"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import numpy  as np\n",
        "import string\n",
        "import re\n",
        "\n",
        "# model libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "#NLP\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "import spacy\n",
        "from spacy.symbols import nsubj, VERB\n",
        "nlp = spacy.load(\"nl_core_news_sm\")\n",
        "#from spacy.lang.nl.stop_words import STOP_WORDS\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset for full_data\n",
        "df = pd.read_csv(\"classification_cleaned_data.csv\")\n",
        "\n",
        "# Loading manual labbeled datasets\n",
        "df_test_q = pd.read_csv(\"question_training_dataset.csv\")\n",
        "df_test_c = pd.read_csv(\"concern_training_dataset.csv\")\n",
        "df_test_d = pd.read_csv(\"doubt_training_dataset.csv\")"
      ],
      "metadata": {
        "id": "Beg8dUtbo_QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into training and remaining data\n",
        "train_data, remaining_data = train_test_split(df, test_size=0.4, random_state=42)  # 60% training, 40% remaining\n",
        "val_data, test_data = train_test_split(remaining_data, test_size=0.5, random_state=42)  # 20% validation, 20% test"
      ],
      "metadata": {
        "id": "AY17vB6RpKuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making sure that the datatype is string\n",
        "train_data['clean_text'] = train_data['clean_text'].astype(str)\n",
        "test_data['clean_text'] = test_data['clean_text'].astype(str)\n",
        "val_data['clean_text'] = val_data['clean_text'].astype(str)"
      ],
      "metadata": {
        "id": "DJTHcD3TpQvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize a TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer(min_df=2, max_df=0.4, norm='l2')\n",
        "\n",
        "# Fit the vectorizer on the training data and transform the training data\n",
        "X_train = vectorizer.fit_transform(train_data['clean_text'])\n",
        "y_train_q = train_data['is_question'].astype(int)\n",
        "y_train_c = train_data['is_concern'].astype(int)\n",
        "y_train_d = train_data['is_doubt'].astype(int)\n",
        "\n",
        "# Transform the validation and test data\n",
        "X_val = vectorizer.transform(val_data['clean_text'])\n",
        "y_val_q = val_data['is_question'].astype(int)\n",
        "y_val_c = val_data['is_concern'].astype(int)\n",
        "y_val_d = val_data['is_doubt'].astype(int)\n",
        "\n",
        "# Will not be used, since manual labelled dataset will be used as test set\n",
        "X_test = vectorizer.transform(test_data['clean_text'])\n",
        "\n",
        "# This is for the prints of the False positives and negatives\n",
        "X_test_original_q = df_test_q['clean_text']\n",
        "X_test_original_c = df_test_c['clean_text']\n",
        "X_test_original_d = df_test_d['clean_text']\n",
        "\n",
        "y_test_q = test_data['is_question'].astype(int)\n",
        "y_test_c = test_data['is_concern'].astype(int)\n",
        "y_test_d = test_data['is_doubt'].astype(int)\n",
        "\n",
        "X_test_manual_q = vectorizer.transform(df_test_q['clean_text'])\n",
        "X_test_manual_c = vectorizer.transform(df_test_c['clean_text'])\n",
        "X_test_manual_d = vectorizer.transform(df_test_d['clean_text'])\n",
        "\n",
        "y_test_manual_q = df_test_q['is_question'].astype(int)\n",
        "y_test_manual_c = df_test_c['is_concern'].astype(int)\n",
        "y_test_manual_d = df_test_d['is_doubt'].astype(int)\n",
        "\n",
        "# Checking the size of the train and validation set\n",
        "print(\"Training Set Size:\", X_train.shape)\n",
        "print(\"Validation Set Size:\", X_val.shape)"
      ],
      "metadata": {
        "id": "qCWWuAxgpT35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Oversampling"
      ],
      "metadata": {
        "id": "fi4QJKl6BkIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smote = SMOTE(random_state=42)\n",
        "X_train_q_res, y_train_q_res = smote.fit_resample(X_train, y_train_q)\n",
        "X_train_c_res, y_train_c_res = smote.fit_resample(X_train, y_train_c)\n",
        "X_train_d_res, y_train_d_res = smote.fit_resample(X_train, y_train_d)\n",
        "\n",
        "# Convert the result to a pandas Series to use value_counts()\n",
        "y_train_q_res_series = pd.Series(y_train_q_res)\n",
        "y_train_c_res_series = pd.Series(y_train_c_res)\n",
        "y_train_d_res_series = pd.Series(y_train_d_res)\n",
        "\n",
        "# Count the occurrences of each class for question\n",
        "class_counts_q = y_train_q_res_series.value_counts()\n",
        "print(class_counts_q)\n",
        "\n",
        "# Count the occurrences of each class for concern\n",
        "class_counts_c = y_train_c_res_series.value_counts()\n",
        "print(class_counts_c)\n",
        "\n",
        "# Count the occurrences of each class for doubt\n",
        "class_counts_d = y_train_d_res_series.value_counts()\n",
        "print(class_counts_d)\n"
      ],
      "metadata": {
        "id": "eBlM9s1TqY0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grid_search_and_evaluate_2(X_train, y_train, X_test, y_test, target_name):\n",
        "    # Defining the parameter grid for grid search\n",
        "    param_grid = {'alpha': [0.01, 0.1, 0.5, 1.0, 2.0, 5.0]}\n",
        "\n",
        "    # Setting up the grid search with cross-validation\n",
        "    grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=5, scoring='f1_macro')\n",
        "\n",
        "    # Fit the grid search on the resampled training data\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Get the best parameters\n",
        "    print(f\"Best parameters for {target_name}: \", grid_search.best_params_)\n",
        "\n",
        "    # Train the best model\n",
        "    best_nb_model = grid_search.best_estimator_\n",
        "    best_nb_model.fit(X_train, y_train)\n",
        "\n",
        "    # Perform cross-validation on the resampled training data\n",
        "    cv_scores = cross_val_score(best_nb_model, X_train, y_train, cv=10, scoring='f1_macro')\n",
        "    print(f\"Cross-validation scores for {target_name}: {cv_scores}\")\n",
        "    print(f\"Mean cross-validation score for {target_name}: {np.mean(cv_scores)}\")\n",
        "\n",
        "    # Predict and evaluate on the test set\n",
        "    y_preds = best_nb_model.predict(X_test)\n",
        "    print(f\"Classification report for {target_name}:\")\n",
        "    print(classification_report(y_test, y_preds, digits=4))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_preds)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['False', 'True'], yticklabels=['False', 'True'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title(f'Confusion Matrix for {target_name}')\n",
        "    plt.show()\n",
        "\n",
        "    # Identify false positives and false negatives\n",
        "    false_positives = np.where((y_preds == 1) & (y_test == 0))[0]\n",
        "    false_negatives = np.where((y_preds == 0) & (y_test == 1))[0]\n",
        "\n",
        "    # Change the list into question, concern and doubt\n",
        "    X_test_list = X_test_original_q.tolist()\n",
        "    # X_test_list = X_test_original_c.tolist()\n",
        "    # X_test_list = X_test_original_d.tolist()\n",
        "\n",
        "    # Making a df for identifying False positives and negatives\n",
        "    df_test = pd.DataFrame({'clean_text': X_test_list, 'actual': y_test, 'predicted': y_preds})\n",
        "\n",
        "    # Extract false positives and false negatives\n",
        "    false_positive_samples = df_test.iloc[false_positives]\n",
        "    false_negative_samples = df_test.iloc[false_negatives]\n",
        "\n",
        "    # Display 10 of the false positives\n",
        "    print(\"False Positives:\")\n",
        "    print(false_positive_samples.head(10))\n",
        "\n",
        "    # Display 10 of the false negatives\n",
        "    print(\"\\nFalse Negatives:\")\n",
        "    print(false_negative_samples.head(10))\n",
        "\n",
        "    # Plot cross-validation scores to detect overfitting\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(1, 11), cv_scores, marker='o', linestyle='--')\n",
        "    plt.title(f'Cross-Validation F1 Scores for {target_name}')\n",
        "    plt.xlabel('Fold')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "485zLCz-qj4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing the function on every category\n",
        "grid_search_and_evaluate_2(X_train_q_res, y_train_q_res, X_test_manual_q, y_test_manual_q, \"is_question\")\n",
        "grid_search_and_evaluate_2(X_train_c_res, y_train_c_res, X_test_manual_c, y_test_manual_c, 'Concern')\n",
        "grid_search_and_evaluate_2(X_train_d_res, y_train_d_res, X_test_manual_d, y_test_manual_d, 'Doubt')"
      ],
      "metadata": {
        "id": "qLxam5YRqvhW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}